<!DOCTYPE html><html class="appearance-auto" lang="chinese"><head><meta charset="UTF-8"><title>Spark快速大数据分析——Spark的部署与使用（柒）</title><meta name="description" content="YOU CAN REDO"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q || []).push(arguments)},i[r].l=1 * new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'G-E0GBC11CTD', 'neonexusx.github.io');
ga('send', 'pageview');</script><!-- End Google Analytics -->
<!-- Baidu Analytics --><script>var _hmt = _hmt || [];
(function() {
var hm = document.createElement("script");
hm.src = "//hm.baidu.com/hm.js?" + '1c102c8d6549c8317b34036a36f85904';
var s = document.getElementsByTagName("script")[0];
s.parentNode.insertBefore(hm, s);
})();</script><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><link rel="stylesheet" href="/style/common/jquery.fancybox.min.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="Spark快速大数据分析——Spark的部署与使用（柒）


软件环境
强烈提示：
请按照对应配套版本来进行环境配置！

Hadoop-3.2
IDEA 2021.3.2
Spark-3.1.3 搭配  scala版本：2.12  构建方式：IDEA导入本地JAR，Spark库
JDK 8
Hive 3.1.3
MYSQL 8.0.2
WSL 2 Ubuntu20.04

备注：在安装Spark-3.1.3 之前，请首先安装Hadoop3.2。
Spark在WSL2中的部署
windos下的压缩包和Linux下安装的压缩包完全一致，我们将压缩包送入root用户的目录下：

打开命令行，切换到hadoop用户：
su hadoop
切换以后再使用解压命令：
hadoop@LAPTOP-74GAR8S9:/us.."><script src="//unpkg.com/valine/dist/Valine.min.js"></script><meta name="generator" content="Hexo 7.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">NeoNexus's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Spark快速大数据分析——Spark的部署与使用（柒）</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">Spark快速大数据分析——Spark的部署与使用（柒）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">软件环境</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">Spark在WSL2中的部署</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">使用IDEA连接WSL2运行Spark代码</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link"><span class="toc-text">错误一</span></a></li><li class="toc-item toc-level-4"><a class="toc-link"><span class="toc-text">错误二</span></a></li><li class="toc-item toc-level-4"><a class="toc-link"><span class="toc-text">错误三</span></a></li><li class="toc-item toc-level-4"><a class="toc-link"><span class="toc-text">错误四</span></a></li></ol></li></ol></li></ol></li></ol></div><div class="column is-9"><header class="my-4"><a href="/tags/Spark"><i class="tag post-item-tag">Spark</i></a><a href="/tags/Hadoop"><i class="tag post-item-tag">Hadoop</i></a><a href="/tags/Scala"><i class="tag post-item-tag">Scala</i></a><a href="/tags/Hive"><i class="tag post-item-tag">Hive</i></a></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">Spark快速大数据分析——Spark的部署与使用（柒）</h1><time class="has-text-grey" datetime="2024-10-21T16:00:00.000Z">2024-10-22</time><article class="mt-2 post-content"><h1><span id="spark快速大数据分析spark的部署与使用柒">Spark快速大数据分析——Spark的部署与使用（柒）</span></h1>
<img src="https://s2.loli.net/2024/09/29/I861KJRCQPwropA.jpg" alt="[lab.magiconch.com][福音戰士標題生成器]-1727613037515" style="zoom:50%;">
<img src="https://s2.loli.net/2023/09/18/zXu5EpoCmKH8FiJ.jpg" alt="标准监督" style="zoom: 50%;">
<h2><span id="软件环境">软件环境</span></h2>
<p><strong>强烈提示：<br>
请按照对应配套版本来进行环境配置！</strong></p>
<ul>
<li><strong>Hadoop-3.2</strong></li>
<li><strong>IDEA 2021.3.2</strong></li>
<li><strong>Spark-3.1.3 搭配  scala版本：2.12  构建方式：IDEA导入本地JAR，Spark库</strong></li>
<li><strong>JDK 8</strong></li>
<li><strong>Hive 3.1.3</strong></li>
<li><strong>MYSQL 8.0.2</strong></li>
<li><strong>WSL 2 Ubuntu20.04</strong></li>
</ul>
<p><strong>备注：在安装Spark-3.1.3 之前，请首先安装Hadoop3.2。</strong></p>
<h2><span id="spark在wsl2中的部署">Spark在WSL2中的部署</span></h2>
<p>windos下的压缩包和Linux下安装的压缩包完全一致，我们将压缩包送入root用户的目录下：</p>
<p><img src="https://s2.loli.net/2024/09/29/JbSr6AdRtLPMVsw.png" alt="0-9"></p>
<p>打开命令行，切换到hadoop用户：</p>
<p><code>su hadoop</code></p>
<p>切换以后再使用解压命令：</p>
<pre><code class="language-shell">hadoop@LAPTOP-74GAR8S9:/usr/local$ sudo tar -zvxf /home/jszszzy/spark/spark-3.1.3-bin-hadoop3.2.tgz  -C /usr/local/
</code></pre>
<p>切换目录到解压目录下：</p>
<pre><code class="language-shell">hadoop@LAPTOP-74GAR8S9:/usr/local$ cd /usr/local
</code></pre>
<p>修改对应目录的名字：</p>
<pre><code class="language-shell">hadoop@LAPTOP-74GAR8S9:/usr/local$ sudo mv ./spark-3.1.3-bin-hadoop3.2/ ./spark
</code></pre>
<p>增加用户组权限：</p>
<pre><code class="language-shell">hadoop@LAPTOP-74GAR8S9:/usr/local$ sudo chown -R hadoop:hadoop ./spark
</code></pre>
<p>进入目录修改配置文件：</p>
<pre><code class="language-shell">hadoop@LAPTOP-74GAR8S9:/usr/local$ cd spark/
hadoop@LAPTOP-74GAR8S9:/usr/local/spark$ cp ./conf/spark-env.sh.template ./conf/spark-env.sh
</code></pre>
<p>使用vim修改配置文件，添加以下内容：</p>
<pre><code class="language-shell">hadoop@LAPTOP-74GAR8S9:/usr/local/spark$ vim ./conf/spark-env.sh
</code></pre>
<p>添加hadoop支持：</p>
<pre><code class="language-shell">export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)
</code></pre>
<p>有了上面的配置信息以后，Spark就可以把数据存储到Hadoop分布式文件系统HDFS中，也可以从HDFS中读取数据。如果没有配置上面信息，Spark就只能读写本地数据，无法读写HDFS数据。配置完成后就可以直接使用，不需要像Hadoop运行启动命令。</p>
<p>在测试前请先打开ssh和hadoop还有hive：</p>
<p>打开ssh：</p>
<pre><code>hadoop@LAPTOP-74GAR8S9:/usr/local/hadoop$ sudo service ssh restart
[sudo] password for hadoop:
 * Restarting OpenBSD Secure Shell server sshd                                     [ OK ]
</code></pre>
<p>打开hadoop：</p>
<pre><code class="language-shell">hadoop@LAPTOP-74GAR8S9:/home/jszszzy$ cd /usr/local/hadoop
hadoop@LAPTOP-74GAR8S9:/usr/local/hadoop$ ./sbin/start-dfs.sh
Starting namenodes on [localhost]
</code></pre>
<p>如果有需要还需要打开Mysql和hive，具体可参考hive安装篇。</p>
<p>通过运行Spark自带的示例，验证Spark是否安装成功。</p>
<pre><code class="language-shell">cd /usr/local/spark
bin/run-example SparkPi 2&gt;&amp;1 | grep "Pi is"
</code></pre>
<p>得到pi=31.4即可。</p>
<p><img src="https://s2.loli.net/2024/09/29/S9T7PGYWFn6gwV8.png" alt="0"></p>
<h2><span id="使用idea连接wsl2运行spark代码">使用IDEA连接WSL2运行Spark代码</span></h2>
<p>打开IDEA创建一个sbt项目，如果你不会创建，请参考专栏对应文章。</p>
<p>找到编辑编译配置项目，具体如下图：</p>
<p><img src="https://s3.bmp.ovh/imgs/2022/06/17/ffa1807c64c247bd.png" alt></p>
<p>打开以后找到如下选项：</p>
<p><img src="https://s2.loli.net/2024/09/29/cDXWGsx2K6wrjhL.png" alt="0-11"></p>
<p>打开Run on选项选择新建运行环境，并选择WSL，选择会会让你配置wsl的运行环境：</p>
<p>他自动检测到了WSL的JDK，实际上安装了两个JDK，应该使用Spark所要的jdk。</p>
<p><img src="https://s2.loli.net/2024/09/29/lDn59uyh2ZbHAKX.png" alt="0-13"></p>
<p>我们点击next，手动配置一下。</p>
<p><img src="https://s2.loli.net/2024/09/29/jzn9kJQPtxWbm7R.png" alt="0-14"></p>
<p>具体怎么填写，可以参考我的。</p>
<p><img src="https://s2.loli.net/2024/09/29/Hdrjf7SOoIcWkgq.png" alt="0-16"></p>
<p>打开Run on选项将Local machine替换为WSL2：</p>
<p><img src="https://s2.loli.net/2024/09/29/yI9i8wa6VtgJWlD.png" alt="0-12"></p>
<p>此时应该就可以愉快的运行了。</p>
<p>需要注意的是你需要先将hadoop和ssh服务都开启才能正常运行，如何打开服务，请参考hadoop配置篇。</p>
<p>以下提供一个参考代码用于检测运行;</p>
<pre><code class="language-scala">package com.jszszzy.test
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.IntegerType

object Test {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("test")
      .enableHiveSupport()
      .getOrCreate()
      }
}
</code></pre>
<p>对应的配置项如下，具体的信息需要你那个修改：</p>
<pre><code class="language-scala">ThisBuild / version := "0.1.0-SNAPSHOT"

ThisBuild / scalaVersion := "2.12.10"

libraryDependencies ++= Seq(
  	"org.apache.spark" %% "spark-hive" % "3.1.3",
	"org.apache.spark" %% "spark-core" % "3.1.3",
  	"org.apache.spark" %% "spark-sql" % "3.1.3"
)
lazy val root = (project in file("."))
  .settings(
    name := "test",
    idePackagePrefix := Some("com.jszszzy.test")
 )
</code></pre>
<p>当然你如果测试的时候出现以下错误，请按我的修改：</p>
<h4><span id="错误一">错误一</span></h4>
<p><img src="D:%5C%E6%96%87%E7%AB%A0%5CSpark%5CSpark(7).assets%5C0-17.png" alt="0-17"></p>
<pre><code class="language-shell">Exception in thread "main" java.lang.IllegalArgumentException: System memory 462422016 must be at least 471859200. Please increase heap size using the --driver-memory option or spark.driver.memory in Spark configuration.
	at org.apache.spark.memory.UnifiedMemoryManager$.getMaxMemory(UnifiedMemoryManager.scala:221)
	at org.apache.spark.memory.UnifiedMemoryManager$.apply(UnifiedMemoryManager.scala:201)
	at org.apache.spark.SparkEnv$.create(SparkEnv.scala:340)
	at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:189)
	at org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:277)
	at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:458)
</code></pre>
<h4><span id="错误二">错误二</span></h4>
<pre><code class="language-shell">org.apache.spark.SparkException: A master URL must be set in your configuration
	at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:394)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2672)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:945)
	at scala.Option.getOrElse(Option.scala:189)
</code></pre>
<p>这是因为JVM分配默认的内存有点小，并且我们没有设置运行的主节点，我们为其添加以下指令：</p>
<pre><code class="language-java">-Dspark.master=local[*] -Xmx3g
</code></pre>
<p>添加的位置如下：</p>
<p><img src="https://s2.loli.net/2024/09/29/mcn6YB3JRH4Labh.png" alt="0-18"></p>
<p><img src="https://s2.loli.net/2024/09/29/VeK6njlJRdQL7c1.png" alt="0-19"></p>
<h4><span id="错误三">错误三</span></h4>
<p>Hive支持失败。</p>
<pre><code class="language-shell">ERROR ApplicationMaster:94 - User class threw exception: java.lang.IllegalArgumentException: Unable to instantiate SparkSession with Hive support because Hive classes are not found.
java.lang.IllegalArgumentException: Unable to instantiate SparkSession with Hive support because Hive classes are not found
</code></pre>
<p>问题不大请将sbt的依赖按照上文修改一摸一样即可，不要添加”provided“选项。</p>
<h4><span id="错误四">错误四</span></h4>
<pre><code class="language-shell">Command line is too long. In order to reduce its length classpath file can be used. Would you like to enable classpath file mode for all run configurations of your project? Enable
</code></pre>
<p>在stackoverflow上找到了解决办法：</p>
<p><img src="https://s2.loli.net/2024/09/29/zBQZi5YSJ1muTId.png" alt="0-20"></p>
<p><img src="https://s2.loli.net/2024/09/29/8CN56yOHUdajFfP.png" alt="0-21"></p>
<p>原回答链接：<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/6381213/idea-10-5-command-line-is-too-long%E3%80%82">https://stackoverflow.com/questions/6381213/idea-10-5-command-line-is-too-long。</a></p>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2024/10/22/Spark(6)/" title="Spark快速大数据分析——Hive的部署与使用（陆）"><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">Previous: Spark快速大数据分析——Hive的部署与使用（陆）</span></a><a class="button is-default" href="/2024/10/21/MOS%E7%AE%A1%20%E2%80%94%E2%80%94%20%E5%BF%AB%E9%80%9F%E5%A4%8D%E8%8B%8F%E5%BA%94%E7%94%A8%E7%AC%94%E8%AE%B0%5B%E5%8E%9F%E7%90%86%E7%AF%87%5D/" title="MOS管 —— 快速复苏应用笔记[参数与应用篇]"><span class="has-text-weight-semibold">Next: MOS管 —— 快速复苏应用笔记[参数与应用篇]</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container" id="vcomments" data-comment_valine_id="hUPsiqArOXp6TNWbIGsRugoz-gzGzoHsz" data-comment_valine_key="bMOEIPDsFffDM5KYhZcZFDwr"></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com//"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/NeoNexusX"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com//"><i class="iconfont icon-ins"></i></a><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--><a title="facebook" target="_blank" rel="noopener nofollow" href="//www.facebook.com//"><i class="iconfont icon-tian7_facebook"></i></a></section><p><span>Copyright ©</span><span> NeoNexus 2025</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span>Puravida & FreeWill</span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/jquery-3.6.1.min.js"></script><script src="/js/jquery-fancybox.min.js"></script><script src="/js/img_zoom.js"></script><script src="/js/post.js"></script><script type="text/javascript">window.MathJax = {
  tex: {
    inlineMath: [['$', '$']],
    displayMath: [['$$', '$$']],
    processEscapes: true,
    tags: 'ams' // 如果需要支持自动编号
 },
  options: {
    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    enableMenu: true,
  },
  startup: {
    ready: () => {
      MathJax.startup.defaultReady();
      MathJax.startup.promise.then(() => {
        // 处理公式渲染完成后的回调函数
        document.querySelectorAll('.MathJax').forEach((el) => {
          el.parentNode.classList.add('has-jax');
        });
      });
    }
  }
};</script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script src="https://unpkg.com/mermaid@10.6.1/dist/mermaid.min.js"></script><script>function initMermaid() {
  if (window.mermaid) {
    mermaid.initialize({ theme: 'forest' });
  }
}</script></body></html>