<!DOCTYPE html><html class="appearance-auto" lang="chinese"><head><meta charset="UTF-8"><title>Spark快速大数据分析——Spark基础（肆）</title><meta name="description" content="YOU CAN REDO"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q || []).push(arguments)},i[r].l=1 * new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'G-E0GBC11CTD', 'neonexusx.github.io');
ga('send', 'pageview');</script><!-- End Google Analytics -->
<!-- Baidu Analytics --><script>var _hmt = _hmt || [];
(function() {
var hm = document.createElement("script");
hm.src = "//hm.baidu.com/hm.js?" + '1c102c8d6549c8317b34036a36f85904';
var s = document.getElementsByTagName("script")[0];
s.parentNode.insertBefore(hm, s);
})();</script><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><link rel="stylesheet" href="/style/common/jquery.fancybox.min.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="Spark快速大数据分析——Spark基础（肆）


软件环境
强烈提示：
请按照对应配套版本来进行环境配置！

Hadoop-3.2
IDEA 2021.3.2
Spark-3.1.3 搭配  scala版本：2.12  构建方式：IDEA导入本地JAR，Spark库
Spark-3.2.1 搭配  scala版本 ：2.13.8 构建方式 ：SBT自动导入依赖
JDK 8



Spark的分布式执行

Spark驱动器
SparkSession
集群管理器
Spark执行器
部署模式
分布式数据与分区


深入理解Spark的基础概念

应用
SparkSession
作业
执行阶段
任务
Spark的操作

转化操作与惰性求值
行动操作


Spark的查询计划
窄转化与宽转化




[TOC]
.."><script src="//unpkg.com/valine/dist/Valine.min.js"></script><meta name="generator" content="Hexo 7.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">NeoNexus's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Spark快速大数据分析——Spark基础（肆）</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">Spark快速大数据分析——Spark基础（肆）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">软件环境</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">Spark的分布式执行</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">Spark驱动器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">SparkSession</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">集群管理器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">Spark执行器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">部署模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">分布式数据与分区</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">深入理解Spark的基础概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">SparkSession</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">作业</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">执行阶段</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">任务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">Spark的操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link"><span class="toc-text">转化操作与惰性求值</span></a></li><li class="toc-item toc-level-4"><a class="toc-link"><span class="toc-text">行动操作</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">Spark的查询计划</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">窄转化与宽转化</span></a></li></ol></li></ol></li></ol></div><div class="column is-9"><header class="my-4"><a href="/tags/Spark"><i class="tag post-item-tag">Spark</i></a><a href="/tags/Hadoop"><i class="tag post-item-tag">Hadoop</i></a><a href="/tags/Scala"><i class="tag post-item-tag">Scala</i></a></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">Spark快速大数据分析——Spark基础（肆）</h1><time class="has-text-grey" datetime="2024-10-21T16:00:00.000Z">2024-10-22</time><article class="mt-2 post-content"><h1><span id="spark快速大数据分析spark基础肆">Spark快速大数据分析——Spark基础（肆）</span></h1>
<img src="https://s2.loli.net/2024/09/29/NbsHdGrJ1IkZFVY.jpg" alt="[lab.magiconch.com][福音戰士標題生成器]-1727612362790" style="zoom:50%;">
<img src="https://s2.loli.net/2023/09/18/zXu5EpoCmKH8FiJ.jpg" alt="标准监督" style="zoom: 50%;">
<h2><span id="软件环境">软件环境</span></h2>
<p><strong>强烈提示：<br>
请按照对应配套版本来进行环境配置！</strong></p>
<ul>
<li><strong>Hadoop-3.2</strong></li>
<li><strong>IDEA 2021.3.2</strong></li>
<li><strong>Spark-3.1.3 搭配  scala版本：2.12  构建方式：IDEA导入本地JAR，Spark库</strong></li>
<li><strong>Spark-3.2.1 搭配  scala版本 ：2.13.8 构建方式 ：SBT自动导入依赖</strong></li>
<li><strong>JDK 8</strong></li>
</ul>
<!-- toc -->
<ul>
<li><a href="#spark%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%89%A7%E8%A1%8C">Spark的分布式执行</a>
<ul>
<li><a href="#spark%E9%A9%B1%E5%8A%A8%E5%99%A8">Spark驱动器</a></li>
<li><a href="#sparksession">SparkSession</a></li>
<li><a href="#%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%99%A8">集群管理器</a></li>
<li><a href="#spark%E6%89%A7%E8%A1%8C%E5%99%A8">Spark执行器</a></li>
<li><a href="#%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F">部署模式</a></li>
<li><a href="#%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E4%B8%8E%E5%88%86%E5%8C%BA">分布式数据与分区</a></li>
</ul>
</li>
<li><a href="#%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3spark%E7%9A%84%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5">深入理解Spark的基础概念</a>
<ul>
<li><a href="#%E5%BA%94%E7%94%A8">应用</a></li>
<li><a href="#sparksession-1">SparkSession</a></li>
<li><a href="#%E4%BD%9C%E4%B8%9A">作业</a></li>
<li><a href="#%E6%89%A7%E8%A1%8C%E9%98%B6%E6%AE%B5">执行阶段</a></li>
<li><a href="#%E4%BB%BB%E5%8A%A1">任务</a></li>
<li><a href="#spark%E7%9A%84%E6%93%8D%E4%BD%9C">Spark的操作</a>
<ul>
<li><a href="#%E8%BD%AC%E5%8C%96%E6%93%8D%E4%BD%9C%E4%B8%8E%E6%83%B0%E6%80%A7%E6%B1%82%E5%80%BC">转化操作与惰性求值</a></li>
<li><a href="#%E8%A1%8C%E5%8A%A8%E6%93%8D%E4%BD%9C">行动操作</a></li>
</ul>
</li>
<li><a href="#spark%E7%9A%84%E6%9F%A5%E8%AF%A2%E8%AE%A1%E5%88%92">Spark的查询计划</a></li>
<li><a href="#%E7%AA%84%E8%BD%AC%E5%8C%96%E4%B8%8E%E5%AE%BD%E8%BD%AC%E5%8C%96">窄转化与宽转化</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->
<p>[TOC]</p>
<h2><span id="spark的分布式执行">Spark的分布式执行</span></h2>
<p>为了方便理解概念，我将完整的交互关系图画了出来，可以一边看文章配合着这张图来理解：</p>
<p><img src="https://s2.loli.net/2024/09/29/FLwSZTncIHJm3ez.jpg" alt="1_37e"></p>
<h3><span id="spark驱动器">Spark驱动器</span></h3>
<p>作为Spark应用中负责初始化的SparkSession的部分，Spark驱动器扮演着多个角色：</p>
<ul>
<li><strong>与集群管理器打交道</strong>，<strong>向集群管理器申请Spark执行器（JVM）所需要的资源</strong>；</li>
<li><strong>将Spark操作转换为DAG运算，并负责调度，还要将计算分成任务分发到Spark执行器上</strong>。</li>
</ul>
<h3><span id="sparksession">SparkSession</span></h3>
<p>在Spark2.0之中，SparkSession是所有Spark操作和数据的统一入口。它不仅封装了Spark程序之前的各种入口，还让Spark变得更加简单。</p>
<p>2.x和1.x版本的区别在哪里呢？</p>
<p>在1.x版本你需要创建多个上下文对象（分别对数据进行处理，sql查询等），这会让代码显得很繁琐。但在Spark2.x中你只需要为每个JVM创建一个Sparksession即可，就可以通过Sparksession调用全部对象。</p>
<p>在Spark2.x版本中虽然Sparksession已经包含了其中操作的各种对象例如使用SparkContext和SQLContext产生的对象，你也可以直接旧版的1.x的代码来使用这些类和对象，也就是说2.x兼容1.x版本的代码。</p>
<p>通过Sparksession入口，可以创建JVM运行时的参数、定义DataFrame或Dataset、从数据源读取数据、访问数据库的元数据，并发起SQL查询。SparkSession为所有的Spark功能提供了统一的入口。</p>
<h3><span id="集群管理器">集群管理器</span></h3>
<p>集群管理器负责管理和分配各个节点的资源，以用来Spark应用的执行。</p>
<p>目前Spark支持4种集群管理器：</p>
<ul>
<li>Spark自带的独立集群管理器</li>
<li>Apache Hadoop YARN</li>
<li>Apache Mesos</li>
<li>Kubernetes</li>
</ul>
<h3><span id="spark执行器">Spark执行器</span></h3>
<p>Spark执行器在各个节点上进行工作，<strong>执行器与驱动器程序通信</strong>，<strong>并负责在工作节点上执行任务</strong>。<strong>在大多数部署模式，每个节点上的只有一个执行器。</strong></p>
<h3><span id="部署模式">部署模式</span></h3>
<p>目前spark支持的部署模式如下：</p>
<img src="https://s2.loli.net/2024/09/29/tdMcbAOW8VigraU.png" alt="9" style="zoom:67%;">
<p>常用的就是本地模式和独立模式。</p>
<h3><span id="分布式数据与分区">分布式数据与分区</span></h3>
<p>实际上物理数据是以分区的形式分布在<strong>HDFS</strong>或者云储存上的，数据分区遍布整个物理集群，而Spark将每个分区在逻辑上抽象为内存中的一个Dataframe（和结构化数据结构不相同），根据本地要求在分配任务的时候根据要求的数据分区与各Spark执行器上在网络上的远近，将数据分配到最近的Spark执行器上，将数据分区的分布式结构可以让Spark执行器只处理靠近自己的数据，从而最小化网络带宽的使用率。也就是说，执行器的每个核心都能分到自己要处理数据的分区。</p>
<p>如下图展示了Spark的分区数据结构：</p>
<p><img src="https://s2.loli.net/2024/09/29/Ahf81IDBSlu2gHn.png" alt="10"></p>
<h2><span id="深入理解spark的基础概念">深入理解Spark的基础概念</span></h2>
<h3><span id="应用">应用</span></h3>
<p>使用Spark的API构建基于Spark的用户程序。</p>
<p>一般由一个驱动器和集群内多个执行器组成。</p>
<h3><span id="sparksession">SparkSession</span></h3>
<p>SparkSession对象提供与下层Spark功能交互的入口。它允许用户用Spark API编写Spark程序。在交互式Spark shell种，Spark驱动器已经初始化了一个SparkSession对象，但在Spark应用中，你需要自行创建SparkSession对象。</p>
<p>所有Spark应用的核心都是Spark驱动器程序，他要负责创建Spark Session对象当使用Spark shell的时候驱动器是shell的一部分，SparkSeesion对象已经自动创建好了，在启动Spark-shell的时候可以看见：</p>
<p><img src="https://s2.loli.net/2024/09/29/PZF4zJnpf1RxH8s.png" alt="1"></p>
<p>在前边windos下Spark的启动都是以本地模式启动的 Spark shell，因此所有操作都会在本地的单个JVM中执行。如果要在集群中启动Spark shell来并行分析数据，其实和本地模式差不多，使用spark-shell --help来展示如何连接：</p>
<p><img src="https://s2.loli.net/2024/09/29/hfXPEbrjAyNdags.png" alt="2"></p>
<p>下图展示了Spark分布式集群管理器是如何通过驱动器实现各节点连接的</p>
<p><img src="https://s2.loli.net/2024/09/29/ZUqNKnly37dmktb.png" alt="3"></p>
<h3><span id="作业">作业</span></h3>
<p>在Spark-shell进行交互式会话中，<strong>驱动器会将Spark应用转为一个或者多个Spark作业</strong>，如图所示：</p>
<p><img src="https://s2.loli.net/2024/09/29/D86SsfQrUl7OFME.png" alt="4"></p>
<p>驱动器会自动将每个作业转化为一个DAG（<strong>有向无环图</strong>），这个DAG本质上就是Spark运行的执行计划，每一个节点都有可能是一个或者多个Spark执行阶段，具体什么是执行阶段接着往下看.</p>
<h3><span id="执行阶段">执行阶段</span></h3>
<p>Spark执行阶段可以作为任务计划的一个节点，也就是DAG图中的一个节点，Spark会根据操作能否并行执行或者必须串行执行来创建执行阶段。<strong>不是所有的Spark操作只对应一个阶段，某些操作可能是多个阶段的。</strong></p>
<p><strong>执行阶段之间可能会触发Spark执行器间的数据传输。</strong></p>
<p>下图展示了一个作业如何被分解成一个DAG内多个执行阶段的。</p>
<p><img src="https://s2.loli.net/2024/09/29/dTackD8lmRW9NMO.png" alt="5"></p>
<h3><span id="任务">任务</span></h3>
<p>执行阶段是由Spark任务组成的，这些任务由Spark的各个执行器一起执行。每个任务对应一个<strong>处理器核心</strong>（cpu 物理上的核心或者超线程），并处理一个<strong>数据分区</strong>，从而使Spark的任务执行高度并行化，更重要的是不需要用户自己具体考虑如何具体并行化。</p>
<p>下图展示了Spark执行阶段创建的多个任务：</p>
<p><img src="https://s2.loli.net/2024/09/29/swEJA18OlvkgWjb.png" alt="6"></p>
<h3><span id="spark的操作">Spark的操作</span></h3>
<p>Spark对分布式数据的操作可以分为两种类型:<strong>转化操作和行动操作</strong>。</p>
<h4><span id="转化操作与惰性求值">转化操作与惰性求值</span></h4>
<p>顾名思义，转化操作就是将Spark DataFrame转化为新的DataFrame，而不改变原有的数据，这赋予了DataFrame不可变的属性，并不改变原有的数据，这样就赋予了Dataframe不可变的属性。换句话说，像select()[选择数据]或者filter()[过滤数据]这种操作并不会修改原有的数据，这些操作会将数据返回成一个新的DataFrame。</p>
<p>所有转化操作都是惰性求值的，也就是说，具体结果不会立即计算出来，Spark只是将具体的转化关系作为血缘(lineage）记录下来，记录下来的血缘允许Spark在后续生成执行计划时重新安排要做的转化操作，比如合并多个转化操作，或者优化为不同的执行阶段来提高执行效率。惰性求值是Spark的策略，目的是直到调用行动操作或“碰”到数据（从硬盘上读取或向硬盘写入)时才真正执行行动操作会触发所记录下来的所有转化操作的实际求值。</p>
<h4><span id="行动操作">行动操作</span></h4>
<p>行动操作会触发记录下来的所有转化操作的实际求值。也就是说转化操作会被行动操作所触发。</p>
<p><img src="https://s2.loli.net/2024/09/29/nqUu3i69ycXILNG.png" alt="7"></p>
<p>如上图，在调用A操作前，所有转化操作T都只是被记录下来了。每个操作T都会生成一个新的DataFrame</p>
<p><strong>惰性求值允许Spark通过综合分析整个转化操作链来优化查询。</strong></p>
<p><strong>血缘（lineage）和数据不可变性则让Spark拥有容错性。只需要通过Spark记录下来的血缘就可以重新构建出DataFrame，这样Spark便能够在失败的时候依然有机会恢复！</strong></p>
<p>常用的转化与行动操作：</p>
<img src="https://s2.loli.net/2024/09/29/ag1k8B2hNLuv3rG.png" alt="8" style="zoom: 67%;">
<h3><span id="spark的查询计划">Spark的查询计划</span></h3>
<p>行动操作和转化操作生成了Spark的查询计划，以后会对此进行介绍。调用行动操作之前Spark并不会执行查询计划。只有行动操作会触发查询计划的执行，此时查询计划中包含的的所有记录下来的转化操作才会真正执行。</p>
<h3><span id="窄转化与宽转化">窄转化与宽转化</span></h3>
<p>如前文所述，转化操作是Spark特有的惰性求值操作。惰性求值方案的最大优势是，Spark可以分析整个计算查询，然后针对计算查询过程进行分析优化，到后边我们会举例Spark是如何优化的。这里的优化包括将一些操作连起来放在一个执行阶段中进行管道化执行，或者根据是否需要集群节点进行数据交换或者混洗，将操作分为多个阶段来执行。</p>
<p>根据依赖关系属于窄依赖还是宽依赖可以将转化操作分为两类。如果输出的单个数据分区是由单个输入分区计算得来的，那么这样的转化操作就成为窄转化。比如filter()和contains()就属于窄转化，因为这些操作在每个数据分区是独立的，生成的输出数据分区时不需要跨分区进行操作。</p>
<p>然而，groupby()和orderby（）会产生宽转化操作。对于统计每个分区存在单词“spark”的出现次数，需要从每个执行器上将来自于各个分区的数据进行混洗。具体是如何实现的留到后面再说。</p>
<p><img src="https://s2.loli.net/2024/09/29/zOsunEPx7Jpy2wh.png" alt="18"></p>
<p>以上就是Spark的一些基础概念，下节将介绍Spark代码具体的应用。</p>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2024/10/22/Spark(2)/" title="Spark的WSL环境安装与Hadoop环境配置（贰）"><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">Previous: Spark的WSL环境安装与Hadoop环境配置（贰）</span></a><a class="button is-default" href="/2024/10/22/Spark(5_1)/" title="Spark快速大数据分析——Spark的结构化数据API（伍—分支1）"><span class="has-text-weight-semibold">Next: Spark快速大数据分析——Spark的结构化数据API（伍—分支1）</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container" id="vcomments" data-comment_valine_id="hUPsiqArOXp6TNWbIGsRugoz-gzGzoHsz" data-comment_valine_key="bMOEIPDsFffDM5KYhZcZFDwr"></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com//"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/NeoNexusX"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com//"><i class="iconfont icon-ins"></i></a><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--><a title="facebook" target="_blank" rel="noopener nofollow" href="//www.facebook.com//"><i class="iconfont icon-tian7_facebook"></i></a></section><p><span>Copyright ©</span><span> NeoNexus 2025</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span>Puravida & FreeWill</span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/jquery-3.6.1.min.js"></script><script src="/js/jquery-fancybox.min.js"></script><script src="/js/img_zoom.js"></script><script src="/js/post.js"></script><script type="text/javascript">window.MathJax = {
  tex: {
    inlineMath: [['$', '$']],
    displayMath: [['$$', '$$']],
    processEscapes: true,
    tags: 'ams' // 如果需要支持自动编号
 },
  options: {
    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    enableMenu: true,
  },
  startup: {
    ready: () => {
      MathJax.startup.defaultReady();
      MathJax.startup.promise.then(() => {
        // 处理公式渲染完成后的回调函数
        document.querySelectorAll('.MathJax').forEach((el) => {
          el.parentNode.classList.add('has-jax');
        });
      });
    }
  }
};</script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script src="https://unpkg.com/mermaid@10.6.1/dist/mermaid.min.js"></script><script>function initMermaid() {
  if (window.mermaid) {
    mermaid.initialize({ theme: 'forest' });
  }
}</script></body></html>