<!DOCTYPE html><html class="appearance-auto" lang="chinese"><head><meta charset="UTF-8"><title>Spark快速大数据分析——Spark的结构化数据API（伍）</title><meta name="description" content="YOU CAN REDO"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q || []).push(arguments)},i[r].l=1 * new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'G-E0GBC11CTD', 'neonexusx.github.io');
ga('send', 'pageview');</script><!-- End Google Analytics -->
<!-- Baidu Analytics --><script>var _hmt = _hmt || [];
(function() {
var hm = document.createElement("script");
hm.src = "//hm.baidu.com/hm.js?" + '1c102c8d6549c8317b34036a36f85904';
var s = document.getElementsByTagName("script")[0];
s.parentNode.insertBefore(hm, s);
})();</script><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><link rel="stylesheet" href="/style/common/jquery.fancybox.min.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="Spark快速大数据分析——Spark的结构化数据API（伍）




软件环境
Spark的结构化数据API

RDD(Resilient Distributed Datasets)弹性分布式数据集

RDD存在的问题
Spark的结构化数据
结构化的关键点与好处
DataFrame API
Spark的基本数据类型
Spark的复杂数据类型






[TOC]
软件环境
强烈提示：
请按照对应配套版本来进行环境配置！

Hadoop-3.2
IDEA 2021.3.2
Spark-3.1.3 搭配  scala版本：2.12  构建方式：IDEA导入本地JAR，Spark库
Spark-3.2.1 搭配  scala版本 ：2.13.8 构建方式 ：SBT自动导入依赖
JDK 8

Spark的结构.."><script src="//unpkg.com/valine/dist/Valine.min.js"></script><meta name="generator" content="Hexo 7.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">NeoNexus's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Spark快速大数据分析——Spark的结构化数据API（伍）</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">Spark快速大数据分析——Spark的结构化数据API（伍）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">软件环境</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">Spark的结构化数据API</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">RDD(Resilient Distributed Datasets)弹性分布式数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link"><span class="toc-text">RDD存在的问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link"><span class="toc-text">Spark的结构化数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link"><span class="toc-text">结构化的关键点与好处</span></a></li><li class="toc-item toc-level-4"><a class="toc-link"><span class="toc-text">DataFrame API</span></a></li><li class="toc-item toc-level-4"><a class="toc-link"><span class="toc-text">Spark的基本数据类型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link"><span class="toc-text">Spark的复杂数据类型</span></a></li></ol></li></ol></li></ol></li></ol></div><div class="column is-9"><header class="my-4"><a href="/tags/Spark"><i class="tag post-item-tag">Spark</i></a><a href="/tags/Hadoop"><i class="tag post-item-tag">Hadoop</i></a><a href="/tags/Scala"><i class="tag post-item-tag">Scala</i></a></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">Spark快速大数据分析——Spark的结构化数据API（伍）</h1><time class="has-text-grey" datetime="2024-10-21T16:00:00.000Z">2024-10-22</time><article class="mt-2 post-content"><h1><span id="spark快速大数据分析spark的结构化数据api伍">Spark快速大数据分析——Spark的结构化数据API（伍）</span></h1>
<img src="https://s2.loli.net/2024/09/29/2FrYNJepTGxCiEs.jpg" alt="[lab.magiconch.com][福音戰士標題生成器]-1727612573373" style="zoom:50%;">
<img src="https://s2.loli.net/2023/09/18/zXu5EpoCmKH8FiJ.jpg" alt="标准监督" style="zoom: 50%;">
<!-- toc -->
<ul>
<li><a href="#%E8%BD%AF%E4%BB%B6%E7%8E%AF%E5%A2%83">软件环境</a></li>
<li><a href="#spark%E7%9A%84%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AEapi">Spark的结构化数据API</a>
<ul>
<li><a href="#rddresilient-distributed-datasets%E5%BC%B9%E6%80%A7%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E9%9B%86">RDD(Resilient Distributed Datasets)弹性分布式数据集</a>
<ul>
<li><a href="#rdd%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98">RDD存在的问题</a></li>
<li><a href="#spark%E7%9A%84%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE">Spark的结构化数据</a></li>
<li><a href="#%E7%BB%93%E6%9E%84%E5%8C%96%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E4%B8%8E%E5%A5%BD%E5%A4%84">结构化的关键点与好处</a></li>
<li><a href="#dataframe-api">DataFrame API</a></li>
<li><a href="#spark%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B">Spark的基本数据类型</a></li>
<li><a href="#spark%E7%9A%84%E5%A4%8D%E6%9D%82%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B">Spark的复杂数据类型</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- tocstop -->
<p>[TOC]</p>
<h2><span id="软件环境">软件环境</span></h2>
<p><strong>强烈提示：<br>
请按照对应配套版本来进行环境配置！</strong></p>
<ul>
<li><strong>Hadoop-3.2</strong></li>
<li><strong>IDEA 2021.3.2</strong></li>
<li><strong>Spark-3.1.3 搭配  scala版本：2.12  构建方式：IDEA导入本地JAR，Spark库</strong></li>
<li><strong>Spark-3.2.1 搭配  scala版本 ：2.13.8 构建方式 ：SBT自动导入依赖</strong></li>
<li><strong>JDK 8</strong></li>
</ul>
<h2><span id="spark的结构化数据api">Spark的结构化数据API</span></h2>
<p>Spark在1.x版本中首次引入了Spark SQL，Spark SQL 引入了表达能力更强的高层操作函数模仿类似于SQL的语法，接着Dataframe作为SchemaRDD的继任者由Spark1.3引入，为后续版本中更多的结构化支持打下了基础，并为Spark查询计算中的高性能操作做铺垫。</p>
<h3><span id="rddresilient-distributed-datasets弹性分布式数据集">RDD(Resilient Distributed Datasets)弹性分布式数据集</span></h3>
<p>在结构化数据API诞生之前，Spark都是使用RDD来完成操作的，首先来讨论一下什么是RDD？</p>
<p>RDD是Spark最基本的抽象。存在三个至关重要的属性：</p>
<ul>
<li>依赖关系</li>
<li>分区</li>
<li>计算函数：Partition =&gt; Iterator[T]</li>
</ul>
<p>这三大属性对于简单的RDD编程API模型来说都是不可或缺的，其他的更高层的功能也给予这三种属性构建。</p>
<p>依赖关系就是上一节所说的血缘，通过依赖关系列表会告诉Spark如何通过输入来构建RDD，同时需要重算的时候Spark可以根据这些依赖关系重新执行操作，重建出RDD。这一属性赋予了RDD容错的弹性。</p>
<p>分区指的是Spark工作的时候以分区为单位，分配到多个执行器上并行运算，再某些情况下如使用HDFS读取信息,Spark会使用分区信息来判断位置远近，这样一来，需要跨网络传输的带宽就少了很多。</p>
<p>最后每一个RDD都需要一个计算函数compute，可用于生成RDD所表示数据的Iterator[T]对象，实际上就是计算完成后的结果。</p>
<h4><span id="rdd存在的问题">RDD存在的问题</span></h4>
<p>RDD传输的计算函数对于Spark来说是不透明的，也就是说Spark不知道你在计算函数里面要干什么。无论是要做连接、过滤、还是聚合，Spark是没有办法具体了解你的传输的lambda表达式具体的操作，实际上让Spark推断的话是很困难的一件事。</p>
<p>这就无法实现前文针对执行计划来做出优化的方式，因为Spark根本不知道你在做什么。无法针对你的具体执行操作来实现优化。</p>
<p>其次是Iterator[T]对象对于Spark来说也是什么都不知道，拿Python的RDD来说，Spark只知道这是一个python的对象，这样就造成了另一个问题。</p>
<p>Spark无法针对数据类型来对数据进行压缩等操作，只能将其不透明的对象转化成字节码传输。</p>
<h4><span id="spark的结构化数据">Spark的结构化数据</span></h4>
<p>针对Spark存在的问题，Spark在2.x版本引入了用于支持结构化数据的方案。其中之一就是为了解决数据操作不透明的问题而诞生的顶层API。</p>
<p>通过引入数据分析常用的模式来表达计算。这些模式使用过滤、选择、计数、聚合、平均和分组等高层操作来进行表示。这样一来，计算的表达更加清晰、简洁了。</p>
<p>通过在DSL（领域特定语言）中使用通用的算子，可以进一步减少计算表达的特异性。Spark所支持的语言都提供了这样一组算子，这些算子让你能够告诉Spark希望计算出来什么数据。这样一来，Spark能够以此为依据为实际执行构建出高效的查询计划，并最终执行。</p>
<h4><span id="结构化的关键点与好处">结构化的关键点与好处</span></h4>
<p>我们首要讨论的就是：</p>
<p>结构化的优点：<strong>表达能力强、简洁、易组合、风格一致。</strong></p>
<p>举个例子：</p>
<p>以下是配套书的代码并未尝试运行，如有错误请以书为准。</p>
<pre><code class="language-python">#Python代码

#创建二元组(nare，age)构成的RDD
dataRDD = sc.parallelize([("Brooke"，20)，("Denny"，31)，(" Jules"，30),("TD"，35)，( "Brooke"，25)])

#调用转化操作rap和reduceByKey，借助lanbda表达式来聚合并计算平均值
agesRDD = (dataRDD
.map(lambda x: (×[0],(×[1]，1)))
.reduceByKey( lambda x, y: (×[0] +y[0],x[1] + y[1]))
.map( lambda x: (×[0],x[1][0]/x[1][1])))
</code></pre>
<p>可以看到实际操作都是使用map等底层数据api通过传入lambda表达式来实现的具体操作，不仅表达能力、可读性差而且跨语言实现起来相同的操作会因为不同语言的语法而导致存在很大的差异，比如使用Scala来实现就完全不相同。</p>
<p>如果使用DataFrame（结构化）呢？</p>
<pre><code class="language-python">#Python代码
fron pyspark.sql inport SparkSessionfron pyspark.sql.functions import avg#用SparkSession创建DataFrame
spark = (SparkSession
.builder
.appNane( "AuthorsAges")
.getOrCreate())
#创建DataFrame
data_df = spark.createDataFrane([("Brooke" ,20)，( "Denny"，31)，(" Jules"，30),("TD"，35)，( "Brooke"，25)]，[ "name" , "age"])

#按名字分组,聚合同名人的年龄,计算平均值
avg_df = data_df.groupBy( "name" ).agg(avg( "age" ))
avg_df.show()#展示执行的最终结果
</code></pre>
<p>跟上一个版本比相比，你应该在没学过的情况下也能大概推断出这一部分代码是干什么的，实际上我们已经通过这些API明确告诉了Spark该去做什么，所以Spark能针对你写的代码进行优化或者重排整体操作被API组合成了一条链式调用。</p>
<p>高度封装API会导致开发者的应用受限不自由，这是经常发生的事情，你在初学Spark的时候就有可能会遇到（使用的不自在），但是目前咱先把这个问题放一放，因为Spark同时支持你使用上边的底层API和高层API混写。</p>
<h4><span id="dataframe-api">DataFrame API</span></h4>
<p>Spark的DataFrame API可以说就是将Pandas库抄过来的，同样是结构化，有格式的，且支持一些特定的操作，就像分布式内存中的表那样，每一列都有名字，每张表都需要通过表的定义来设置数据类型，整型，字符串或者实数等，就单纯来说Dataframe对于人来说就是张表，表头就是表的定义。</p>
<p><img src="https://s2.loli.net/2024/09/29/4nPdrDg7yLiB5sX.png" alt="1"></p>
<h4><span id="spark的基本数据类型">Spark的基本数据类型</span></h4>
<p>Spark的基本数据类型都与对应的内部数据类型，**这些数据类型既可以通过Spark应用来定义，也可以在Spark的表结构来定义。**使用Scala编写代码的时候可以直接将某列的定义声明为String，Byte，Long等。</p>
<p>举个例子：</p>
<p><img src="https://s2.loli.net/2024/09/29/ABjtRIokcvFwmOJ.png" alt="2"></p>
<p>可以看到使用了一个 类型是StringType的变量</p>
<p>注意的是这里使用的实际上是他的构造方法创建的对象，并不是Scala的类型定义的方式<strong>所以是=号</strong>，而这里实际上是将变量绑定到对应spark中StringType的一个叫type的成员类型，并不是真正的字符串。</p>
<p><img src="https://s2.loli.net/2024/09/29/Os5aRAhIBxgi3qF.png" alt="3"></p>
<h4><span id="spark的复杂数据类型">Spark的复杂数据类型</span></h4>
<p>复杂的数据分析不会只有简单的数据类型。有些时候需要格式化，标准化数据类型来作辅助，原数据可能是随意记载的，比如日期，每个人记录格式不统一就需要使用一种标准化的数据类型来将读入数据统一一下。当然还有些数据结构用来构建Dataframe，比如：映射表、数组、结构体。</p>
<p>下表展示了Spark支持的结构化数据类型:</p>
<p><img src="https://s2.loli.net/2024/09/29/1dkGiLEaB6CH83h.png" alt="4"></p>
<p>如果你对Seq或者Map不太熟悉请查看关于scala数据结构的内容。</p>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2024/10/22/Scala%E5%BA%94%E7%94%A8%E7%AC%94%E8%AE%B0/" title="Scala基础笔记"><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">Previous: Scala基础笔记</span></a><a class="button is-default" href="/2024/10/22/Spark(3)/" title="Spark快速大数据分析——Spark的Hadoop配置（叁）"><span class="has-text-weight-semibold">Next: Spark快速大数据分析——Spark的Hadoop配置（叁）</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container" id="vcomments" data-comment_valine_id="hUPsiqArOXp6TNWbIGsRugoz-gzGzoHsz" data-comment_valine_key="bMOEIPDsFffDM5KYhZcZFDwr"></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com//"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/NeoNexusX"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com//"><i class="iconfont icon-ins"></i></a><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--><a title="facebook" target="_blank" rel="noopener nofollow" href="//www.facebook.com//"><i class="iconfont icon-tian7_facebook"></i></a></section><p><span>Copyright ©</span><span> NeoNexus 2025</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span>Puravida & FreeWill</span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/jquery-3.6.1.min.js"></script><script src="/js/jquery-fancybox.min.js"></script><script src="/js/img_zoom.js"></script><script src="/js/post.js"></script><script type="text/javascript">window.MathJax = {
  tex: {
    inlineMath: [['$', '$']],
    displayMath: [['$$', '$$']],
    processEscapes: true,
    tags: 'ams' // 如果需要支持自动编号
 },
  options: {
    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    enableMenu: true,
  },
  startup: {
    ready: () => {
      MathJax.startup.defaultReady();
      MathJax.startup.promise.then(() => {
        // 处理公式渲染完成后的回调函数
        document.querySelectorAll('.MathJax').forEach((el) => {
          el.parentNode.classList.add('has-jax');
        });
      });
    }
  }
};</script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script src="https://unpkg.com/mermaid@10.6.1/dist/mermaid.min.js"></script><script>function initMermaid() {
  if (window.mermaid) {
    mermaid.initialize({ theme: 'forest' });
  }
}</script></body></html>